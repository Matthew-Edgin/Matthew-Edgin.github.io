{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Copy of NLP Feature Engineering Word2vec 1st attempt",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN1wCdc1Gj2dTgO94zKxjQG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matthew-Edgin/Matthew-Edgin.github.io/blob/master/Copy_of_NLP_Feature_Engineering_Word2vec_1st_attempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smh4Bnf_3EfA",
        "outputId": "5c571a60-c7f9-48da-820e-d49f08e69cb5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "import spacy\n",
        "import re\n",
        "import gensim\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\")\n",
        "nltk.download('gutenberg')\n",
        "!python -m spacy download en"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2FCMwcTTfs3"
      },
      "source": [
        "## Function for cleaning the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoM0_hc1TkfA"
      },
      "source": [
        "# Utility function for standard text cleaning\n",
        "def text_cleaner(text):\n",
        "    # Visual inspection identifies a form of punctuation that spaCy doesn't\n",
        "    # recognize: the double dash '--'. Better get rid of it now!\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq4qlurzTq1U"
      },
      "source": [
        "## Load and clean text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_ZI6QkSTvAA"
      },
      "source": [
        "# Load and clean the data\n",
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# The chapter indicator is idiosyncratic\n",
        "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "    \n",
        "alice = text_cleaner(alice)\n",
        "persuasion = text_cleaner(persuasion)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Wj6UkHUWHu"
      },
      "source": [
        "## Parse text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aBsv0CSUZa8"
      },
      "source": [
        "# Parse the cleaned novels. This can take some time.\n",
        "nlp = spacy.load('en')\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEzkc5MPUbVH"
      },
      "source": [
        "## Iterate using the list comprehension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ggkLwjN4Ufue",
        "outputId": "1a702a24-ed99-4163-b18d-a5e4043530d3"
      },
      "source": [
        "# Group into sentences\n",
        "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "\n",
        "# Combine the sentences from the two novels into one DataFrame\n",
        "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
        "sentences.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(I, shall, be, late, !, ')</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   author\n",
              "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
              "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
              "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
              "3                                      (Oh, dear, !)  Carroll\n",
              "4                         (I, shall, be, late, !, ')  Carroll"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f8xJ4CUUrnK"
      },
      "source": [
        "## Lemmatize tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlOOC6aPW7N5"
      },
      "source": [
        "# Get rid of stop words and punctuation,\n",
        "# and lemmatize the tokens\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmB-Mwn3XDON"
      },
      "source": [
        "## Convert text the numeric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boHbGZov0uxe"
      },
      "source": [
        "training word2vec with window size range of 4, 6, 8 and vector sizes 100 and 200."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTuSlwZhXIeh"
      },
      "source": [
        "# train word2vec on the the sentences\n",
        "model1 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=4,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "model2 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=6,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "model3 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=8,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=100,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "model4 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=4,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=200,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "model5 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=6,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=200,\n",
        "    hs=1\n",
        ")\n",
        "\n",
        "model6 = gensim.models.Word2Vec(\n",
        "    sentences[\"text\"],\n",
        "    workers=4,\n",
        "    min_count=1,\n",
        "    window=8,\n",
        "    sg=0,\n",
        "    sample=1e-3,\n",
        "    size=200,\n",
        "    hs=1\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wohayr9e1FHc"
      },
      "source": [
        "word2vec_arr1 = np.zeros((sentences.shape[0],100))\n",
        "word2vec_arr2 = np.zeros((sentences.shape[0],100))\n",
        "word2vec_arr3 = np.zeros((sentences.shape[0],100))\n",
        "word2vec_arr4 = np.zeros((sentences.shape[0],200))\n",
        "word2vec_arr5 = np.zeros((sentences.shape[0],200))\n",
        "word2vec_arr6 = np.zeros((sentences.shape[0],200))\n",
        "\n",
        "for i, sentence in enumerate(sentences[\"text\"]):\n",
        "    word2vec_arr1[i,:] = np.mean([model1[lemma] for lemma in sentence], axis=0)\n",
        "    word2vec_arr2[i,:] = np.mean([model2[lemma] for lemma in sentence], axis=0)\n",
        "    word2vec_arr3[i,:] = np.mean([model3[lemma] for lemma in sentence], axis=0)\n",
        "    word2vec_arr4[i,:] = np.mean([model4[lemma] for lemma in sentence], axis=0)\n",
        "    word2vec_arr5[i,:] = np.mean([model5[lemma] for lemma in sentence], axis=0)\n",
        "    word2vec_arr6[i,:] = np.mean([model6[lemma] for lemma in sentence], axis=0)\n",
        "\n",
        "word2vec_arr1 = pd.DataFrame(word2vec_arr1)\n",
        "word2vec_arr2 = pd.DataFrame(word2vec_arr2)\n",
        "word2vec_arr3 = pd.DataFrame(word2vec_arr3)\n",
        "word2vec_arr4 = pd.DataFrame(word2vec_arr4)\n",
        "word2vec_arr5 = pd.DataFrame(word2vec_arr5)\n",
        "word2vec_arr6 = pd.DataFrame(word2vec_arr6)\n",
        "\n",
        "sentences1 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr1], axis=1)\n",
        "sentences1.dropna(inplace=True)\n",
        "\n",
        "sentences2 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr2], axis=1)\n",
        "sentences2.dropna(inplace=True)\n",
        "\n",
        "sentences3 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr3], axis=1)\n",
        "sentences3.dropna(inplace=True)\n",
        "\n",
        "sentences4 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr4], axis=1)\n",
        "sentences4.dropna(inplace=True)\n",
        "\n",
        "sentences5 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr5], axis=1)\n",
        "sentences5.dropna(inplace=True)\n",
        "\n",
        "sentences6 = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr6], axis=1)\n",
        "sentences6.dropna(inplace=True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7aMhfY-XnWR"
      },
      "source": [
        "## Run after TF-IDF algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZE7CKADXuod",
        "outputId": "a019a448-9823-4d72-e73b-8bdd8ba0ce16"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y1 = sentences1['author']\n",
        "Y2 = sentences2['author']\n",
        "Y3 = sentences3['author']\n",
        "Y4 = sentences4['author']\n",
        "Y5 = sentences5['author']\n",
        "Y6 = sentences6['author']\n",
        "\n",
        "X1 = np.array(sentences1.drop(['text','author'], 1))\n",
        "X2 = np.array(sentences2.drop(['text','author'], 1))\n",
        "X3 = np.array(sentences3.drop(['text','author'], 1))\n",
        "X4 = np.array(sentences4.drop(['text','author'], 1))\n",
        "X5 = np.array(sentences5.drop(['text','author'], 1))\n",
        "X6 = np.array(sentences6.drop(['text','author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size=0.4, random_state=123)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, Y2, test_size=0.4, random_state=123)\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, Y3, test_size=0.4, random_state=123)\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X4, Y4, test_size=0.4, random_state=123)\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X5, Y5, test_size=0.4, random_state=123)\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X6, Y6, test_size=0.4, random_state=123)\n",
        "\n",
        "# Models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "print(\"-----------------------Word2vec Model 1------------------------------\")\n",
        "lr.fit(X_train1, y_train1)\n",
        "rfc.fit(X_train1, y_train1)\n",
        "gbc.fit(X_train1, y_train1)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train1, y_train1))\n",
        "print('\\nTest set score:', lr.score(X_test1, y_test1))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train1, y_train1))\n",
        "print('\\nTest set score:', rfc.score(X_test1, y_test1))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train1, y_train1))\n",
        "print('\\nTest set score:', gbc.score(X_test1, y_test1))\n",
        "\n",
        "print(\"-----------------------Word2vec Model 2------------------------------\")\n",
        "lr.fit(X_train2, y_train2)\n",
        "rfc.fit(X_train2, y_train2)\n",
        "gbc.fit(X_train2, y_train2)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train2, y_train2))\n",
        "print('\\nTest set score:', lr.score(X_test2, y_test2))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train2, y_train2))\n",
        "print('\\nTest set score:', rfc.score(X_test2, y_test2))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train2, y_train2))\n",
        "print('\\nTest set score:', gbc.score(X_test2, y_test2))\n",
        "\n",
        "print(\"-----------------------Word2vec Model 3------------------------------\")\n",
        "lr.fit(X_train3, y_train3)\n",
        "rfc.fit(X_train3, y_train3)\n",
        "gbc.fit(X_train3, y_train3)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train3, y_train3))\n",
        "print('\\nTest set score:', lr.score(X_test3, y_test3))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train3, y_train3))\n",
        "print('\\nTest set score:', rfc.score(X_test3, y_test3))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train3, y_train3))\n",
        "print('\\nTest set score:', gbc.score(X_test3, y_test3))\n",
        "\n",
        "print(\"-----------------------Word2vec Model 4------------------------------\")\n",
        "lr.fit(X_train4, y_train4)\n",
        "rfc.fit(X_train4, y_train4)\n",
        "gbc.fit(X_train4, y_train4)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train4, y_train4))\n",
        "print('\\nTest set score:', lr.score(X_test4, y_test4))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train4, y_train4))\n",
        "print('\\nTest set score:', rfc.score(X_test4, y_test4))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train4, y_train4))\n",
        "print('\\nTest set score:', gbc.score(X_test4, y_test4))\n",
        "\n",
        "print(\"-----------------------Word2vec Model 5------------------------------\")\n",
        "lr.fit(X_train5, y_train5)\n",
        "rfc.fit(X_train5, y_train5)\n",
        "gbc.fit(X_train5, y_train5)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train5, y_train5))\n",
        "print('\\nTest set score:', lr.score(X_test5, y_test5))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train5, y_train5))\n",
        "print('\\nTest set score:', rfc.score(X_test5, y_test5))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train5, y_train5))\n",
        "print('\\nTest set score:', gbc.score(X_test5, y_test5))\n",
        "\n",
        "print(\"-----------------------Word2vec Model 6------------------------------\")\n",
        "lr.fit(X_train6, y_train6)\n",
        "rfc.fit(X_train6, y_train6)\n",
        "gbc.fit(X_train6, y_train6)\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train6, y_train6))\n",
        "print('\\nTest set score:', lr.score(X_test6, y_test6))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train6, y_train6))\n",
        "print('\\nTest set score:', rfc.score(X_test6, y_test6))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train6, y_train6))\n",
        "print('\\nTest set score:', gbc.score(X_test6, y_test6))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------Word2vec Model 1------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7540334855403349\n",
            "\n",
            "Test set score: 0.763013698630137\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9914764079147641\n",
            "\n",
            "Test set score: 0.8091324200913242\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.8770167427701674\n",
            "\n",
            "Test set score: 0.806392694063927\n",
            "-----------------------Word2vec Model 2------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7680365296803653\n",
            "\n",
            "Test set score: 0.7726027397260274\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9917808219178083\n",
            "\n",
            "Test set score: 0.8100456621004566\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.8983257229832572\n",
            "\n",
            "Test set score: 0.8136986301369863\n",
            "-----------------------Word2vec Model 3------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7823439878234398\n",
            "\n",
            "Test set score: 0.7890410958904109\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9917808219178083\n",
            "\n",
            "Test set score: 0.8155251141552512\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.8922374429223744\n",
            "\n",
            "Test set score: 0.8178082191780822\n",
            "-----------------------Word2vec Model 4------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.750076103500761\n",
            "\n",
            "Test set score: 0.7525114155251141\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9917808219178083\n",
            "\n",
            "Test set score: 0.8114155251141553\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.897716894977169\n",
            "\n",
            "Test set score: 0.8123287671232877\n",
            "-----------------------Word2vec Model 5------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7573820395738204\n",
            "\n",
            "Test set score: 0.7625570776255708\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9917808219178083\n",
            "\n",
            "Test set score: 0.8136986301369863\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.9041095890410958\n",
            "\n",
            "Test set score: 0.821917808219178\n",
            "-----------------------Word2vec Model 6------------------------------\n",
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.7713850837138508\n",
            "\n",
            "Test set score: 0.7844748858447489\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9917808219178083\n",
            "\n",
            "Test set score: 0.8191780821917808\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.8937595129375951\n",
            "\n",
            "Test set score: 0.8232876712328767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlguUwxXYM2Z"
      },
      "source": [
        "Not bad scores are lower for all but model 6 with gradient boosting and with less issues with overfitting. The cellalso took significnalty less time to run with a higher ram setting and a TPU."
      ]
    }
  ]
}